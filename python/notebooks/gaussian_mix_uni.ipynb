{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from bayesmixpy import run_mcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BAYESMIX_EXE\"] = '../../build/run_mcmc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([\n",
    "    np.random.normal(loc=3, scale=1, size=100),\n",
    "    np.random.normal(loc=-3, scale=1, size=100),\n",
    "])\n",
    "plt.hist(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian Model\n",
    "\n",
    "\\begin{align*}\n",
    "    y_i \\mid \\theta_i=(\\mu_i, \\sigma^2_i) & \\sim \\mathcal{N}(\\mu_i, \\sigma^2_i) \\\\\n",
    "    \\theta_i \\mid P & \\sim P \\\\\n",
    "    P & \\sim DP(\\alpha G_0) \n",
    "\\end{align*}\n",
    "And $G_0(d\\mu, d\\sigma^2) = \\mathcal N(d\\mu | \\mu_0, \\sigma^2/\\lambda) \\mathcal{IG}(\\sigma^2 | a, b)$\n",
    "\n",
    "We consider different prior specifications\n",
    "\n",
    "### Fixed hyperparameters\n",
    "\n",
    "$\\alpha = 1$\n",
    "\n",
    "$(\\mu_0, \\lambda, a, b) = (0, 0.1, 2, 2)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_params_fix = \"\"\"\n",
    "fixed_value {\n",
    "    totalmass: 1.0\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "g0_params_fix = \"\"\"\n",
    "fixed_values {\n",
    "    mean: 0.0\n",
    "    var_scaling: 0.1\n",
    "    shape: 2.0\n",
    "    scale: 2.0\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior on $\\alpha$ and $\\mu_0$\n",
    "\n",
    "$\\alpha \\sim \\text{Gamma}(2, 2)$\n",
    "\n",
    "$\\mu_0 \\sim \\mathcal{N}(0, 10)$\n",
    "\n",
    "$(\\lambda, a, b) = (0.1, 2, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_params_prior = \"\"\"\n",
    "gamma_prior {\n",
    "  totalmass_prior {\n",
    "    shape: 4.0\n",
    "    rate: 2.0\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "g0_params_meanprior = \"\"\"\n",
    "normal_mean_prior {\n",
    "    mean_prior {\n",
    "        mean: 0.0\n",
    "        var: 10.0\n",
    "    }\n",
    "    var_scaling: 0.1\n",
    "    shape: 2.0\n",
    "    scale: 2.0\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "dp_params = [dp_params_fix, dp_params_prior]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior on all the hyperparameters\n",
    "\n",
    "$\\alpha \\sim \\text{Gamma}(2, 2)$\n",
    "\n",
    "$\\mu_0 \\sim \\mathcal{N}(0, 10)$\n",
    "\n",
    "$\\lambda \\sim \\text{Gamma}(0.2, 0.6)$\n",
    "\n",
    "$a = 1.5$\n",
    "\n",
    "$b \\sim \\text{Gamma}(4, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g0_params_allprior = \"\"\"\n",
    "ngg_prior {\n",
    "    mean_prior {\n",
    "        mean: 5.5\n",
    "        var: 2.25\n",
    "    }\n",
    "    var_scaling_prior {\n",
    "        shape: 0.2\n",
    "        rate: 0.6\n",
    "    }\n",
    "    shape: 1.5\n",
    "    scale_prior {\n",
    "        shape: 4.0\n",
    "        rate: 2.0\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "g0_params = [g0_params_fix, g0_params_meanprior, g0_params_allprior]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "We consider all available algorithms in bayesmix: Neal's Algorithms 2, 3 and 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neal2_algo = \"\"\"\n",
    "algo_id: \"Neal2\"\n",
    "rng_seed: 20201124\n",
    "iterations: 10\n",
    "burnin: 5\n",
    "init_num_clusters: 3\n",
    "\"\"\"\n",
    "\n",
    "neal3_algo = \"\"\"\n",
    "algo_id: \"Neal3\"\n",
    "rng_seed: 20201124\n",
    "iterations: 10\n",
    "burnin: 5\n",
    "init_num_clusters: 3\n",
    "\"\"\"\n",
    "\n",
    "neal8_algo = \"\"\"\n",
    "algo_id: \"Neal8\"\n",
    "rng_seed: 20201124\n",
    "iterations: 10\n",
    "burnin: 5\n",
    "init_num_clusters: 3\n",
    "neal8_n_aux: 3\n",
    "\"\"\"\n",
    "\n",
    "algorithms = [neal2_algo, neal3_algo, neal8_algo]\n",
    "algo_names = [\"Neal2\", \"Neal3\", \"Neal8\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're interested in the predictive density\n",
    "\n",
    "`return_clusters=False, return_num_clusters=False, return_best_clus=False`\n",
    "\n",
    "Observe that the number of iterations is extremely small! In real problems, you might want to set the burnin at least to 1000 iterations and the total number of iterations to at least 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "dens_grid = np.linspace(-10, 10, 1000)\n",
    "\n",
    "for i, dp in enumerate(dp_params):\n",
    "    for j, g0 in enumerate(g0_params):\n",
    "        for k, algo in enumerate(algorithms):\n",
    "            eval_dens = run_mcmc(\n",
    "                \"NNIG\", \"DP\", data, g0, dp, algo, dens_grid,\n",
    "                return_clusters=False, return_num_clusters=False,\n",
    "                return_best_clus=False)[0]\n",
    "            \n",
    "            axes[i, j].plot(dens_grid, np.exp(np.mean(eval_dens, axis=0)),\n",
    "                            label=\"Algo: {0}\".format(algo_names[k]))\n",
    "\n",
    "axes[0, 0].set_ylabel(\"DP - fix\")\n",
    "axes[1, 0].set_ylabel(\"DP - gamma prior\")\n",
    "\n",
    "axes[0, 0].set_title(\"G0 - fix\")\n",
    "axes[0, 1].set_title(\"G0 - mean prior\")\n",
    "axes[0, 2].set_title(\"G0 - NGG prior\")\n",
    "\n",
    "axes[1, 2].legend(ncol=3, bbox_to_anchor=(0.4, -0.2), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the clustering ?\n",
    "\n",
    "We can extract \n",
    "1. The full chain of the cluster allocations\n",
    "2. The chain of the number of clusters\n",
    "3. The \"best\" cluster according to Binder loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neal2_algo = \"\"\"\n",
    "algo_id: \"Neal2\"\n",
    "rng_seed: 20201124\n",
    "iterations: 2000\n",
    "burnin: 1000\n",
    "init_num_clusters: 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , numcluschain, cluschain, bestclus = run_mcmc(\n",
    "    \"NNIG\", \"DP\", data, g0_params_allprior, dp_params_prior, neal2_algo, \n",
    "    dens_grid=None, return_clusters=True, return_num_clusters=True,\n",
    "    return_best_clus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.unique(numcluschain, return_counts=True)\n",
    "plt.bar(x, y / y.sum())\n",
    "plt.xticks(x)\n",
    "plt.title(\"Posterior distribution of the number of clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, alpha=0.3, density=True)\n",
    "for c in np.unique(bestclus):\n",
    "    data_in_clus = data[bestclus == c]\n",
    "    plt.scatter(data_in_clus, np.zeros_like(data_in_clus) + 0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
